import os
import pandas as pd
import unicodedata
import traceback
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError

# -------------------------------
# Configuraci√≥n DB
# -------------------------------
DB_URL = os.getenv("DATABASE_URL") or os.getenv("DATABASE_PUBLIC_URL")
if not DB_URL:
    raise RuntimeError("‚ùå No se encontr√≥ DATABASE_URL ni DATABASE_PUBLIC_URL.")

engine = create_engine(DB_URL)
print(f"üîó Usando URL: {DB_URL}")

# -------------------------------
# Paths de Parquet
# -------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PARQUET_PATH = os.path.join(BASE_DIR, "../data_fuentes/ffmm_merged.parquet")

# üìå Carpeta persistente para el Parquet optimizado (Volume en Railway)
FINAL_PARQUET_PATH = "/data/ffmm_merged.parquet"

# -------------------------------
# Funciones de limpieza de columnas
# -------------------------------
def limpiar_nombre(col):
    col = unicodedata.normalize('NFKD', col).encode('ascii', 'ignore').decode('ascii')
    col = ''.join(c if c.isalnum() else '_' for c in col)
    return col.lower()

def hacer_unicas(cols):
    seen = {}
    nuevas = []
    for c in cols:
        if c not in seen:
            seen[c] = 0
            nuevas.append(c)
        else:
            seen[c] += 1
            nuevas.append(f"{c}_{seen[c]}")
    return nuevas

# -------------------------------
# Pipeline: cargar parquet ‚Üí PostgreSQL
# -------------------------------
def procesar_parquet_por_chunks(ruta_parquet=PARQUET_PATH,
                                tabla_destino="fondos_mutuos",
                                chunk_size=20000):
    print("üöÄ Iniciando carga batch desde parquet...")
    print(f"üìÇ Leyendo parquet: {ruta_parquet}")

    try:
        df = pd.read_parquet(ruta_parquet, engine="pyarrow")
        total = len(df)
        print(f"‚úÖ Dataframe cargado: {total} filas")
        print(f"üìù Columnas originales: {list(df.columns)}")

        df.columns = [limpiar_nombre(c) for c in df.columns]
        df.columns = hacer_unicas(df.columns)
        print(f"üìù Columnas finales: {list(df.columns)}")

    except Exception as e:
        print(f"‚ùå Error al leer parquet: {e}")
        return

    tmp_table = f"{tabla_destino}_tmp"

    try:
        # Borrar tabla temporal previa
        with engine.begin() as conn:
            conn.execute(text(f'DROP TABLE IF EXISTS "{tmp_table}"'))

        # Crear tabla temporal vac√≠a
        print("üìå Creando tabla temporal...")
        with engine.begin() as conn:
            df.iloc[0:0].to_sql(tmp_table, conn, if_exists="replace", index=False)
        print("‚úÖ Tabla temporal creada")

        # Insertar en chunks con logs
        for i in range(0, total, chunk_size):
            chunk = df.iloc[i:i+chunk_size]
            print(f"‚û°Ô∏è Insertando filas {i+1:,} a {i+len(chunk):,} de {total:,}")
            with engine.begin() as conn:
                chunk.to_sql(tmp_table, conn, if_exists="append", index=False)

            with engine.connect() as conn:
                count = conn.execute(text(f'SELECT COUNT(*) FROM "{tmp_table}"')).scalar()
                print(f"üìä Total acumulado en {tmp_table}: {count}")

        # Verificar total
        with engine.connect() as conn:
            total_tmp = conn.execute(text(f'SELECT COUNT(*) FROM "{tmp_table}"')).scalar()
            print(f"üìå Total en {tmp_table}: {total_tmp}")

        if total_tmp != total:
            print("‚ùå El total en la tabla temporal no coincide con el parquet. Abortando swap.")
            return

        # Swap seguro con backup
        with engine.begin() as conn:
            conn.execute(text(f'DROP TABLE IF EXISTS "{tabla_destino}_backup"'))
            conn.execute(text(f'ALTER TABLE "{tabla_destino}" RENAME TO "{tabla_destino}_backup"'))
            conn.execute(text(f'ALTER TABLE "{tmp_table}" RENAME TO "{tabla_destino}"'))

        with engine.connect() as conn:
            final_count = conn.execute(text(f'SELECT COUNT(*) FROM "{tabla_destino}"')).scalar()
            backup_count = conn.execute(text(f'SELECT COUNT(*) FROM "{tabla_destino}_backup"')).scalar()
            print(f"‚úÖ Carga completada.")
            print(f"üìä Total nuevo en {tabla_destino}: {final_count}")
            print(f"üìä Total anterior en {tabla_destino}_backup: {backup_count}")

        # -------------------------------
        # ‚úÖ Generar Parquet optimizado para el Dashboard
        # -------------------------------
        print("üì¶ Generando Parquet optimizado para el Dashboard...")
        df_full = pd.read_sql(f'SELECT * FROM "{tabla_destino}";', engine)
        os.makedirs(os.path.dirname(FINAL_PARQUET_PATH), exist_ok=True)
        df_full.to_parquet(FINAL_PARQUET_PATH, index=False)
        print(f"‚úÖ Parquet generado en {FINAL_PARQUET_PATH} con {df_full.shape[0]:,} filas.")

    except SQLAlchemyError as e:
        print(f"‚ùå Error en el pipeline: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    procesar_parquet_por_chunks()
