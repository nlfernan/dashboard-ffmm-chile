import os
import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError

# Obtener URL de conexi√≥n desde variables de entorno
DB_URL = os.getenv("DB_URL") or os.getenv("DATABASE_URL")
if not DB_URL:
    raise RuntimeError("‚ùå No se encontr√≥ DB_URL ni DATABASE_URL. Verific√° las variables de entorno en Railway.")

engine = create_engine(DB_URL)

def procesar_parquet_por_chunks(ruta_parquet, tabla_destino, chunk_size=50000):
    """
    Carga un parquet en PostgreSQL en chunks para evitar saturar pg_wal.
    Ejecuta VACUUM FULL al final para liberar espacio.
    """
    print(f"üöÄ Iniciando carga batch por chunks desde parquet...")
    print(f"üìÇ Leyendo parquet: {ruta_parquet}")

    try:
        df = pd.read_parquet(ruta_parquet, engine="pyarrow")
        print(f"‚úÖ Dataframe cargado: {len(df)} filas")
        print(f"üìù Columnas: {list(df.columns)}")
    except Exception as e:
        print(f"‚ùå Error al leer parquet: {e}")
        return

    try:
        total = len(df)
        for i in range(0, total, chunk_size):
            chunk = df.iloc[i:i+chunk_size]
            print(f"üîπ Insertando chunk {i//chunk_size + 1}: {len(chunk)} filas")
            try:
                chunk.to_sql(tabla_destino, engine, if_exists="append", index=False, method='multi')
            except SQLAlchemyError as e:
                print(f"‚ö†Ô∏è Error al insertar chunk: {e}")
                break

        # Ejecutar VACUUM FULL ANALYZE al final
        with engine.connect() as conn:
            print("üßπ Ejecutando VACUUM FULL ANALYZE...")
            conn.execute(text(f"VACUUM FULL ANALYZE {tabla_destino};"))
            print("‚úÖ VACUUM completado")
    except Exception as e:
        print(f"‚ùå Error general en procesamiento: {e}")
